Extract claims or references that need citations from the provided text below. Number each claim requiring a reference as a separate line, with no other commentary. The author's beliefs or opinons do not require references. When a sentence contains multiple claims that would require multiple references, enumerate them as multiple individual claims and make sure each includes any context that is important to express the claim.

Abstract

We consider that existing approaches to AI "safety" and "alignment" may not be using the most effective tools, teams, or approaches. We suggest that an alternative and better approach to the problem may be to treat alignment as a social science problem, since the social sciences enjoy a rich toolkit of models for understanding and aligning motivation and behavior, much of which could be repurposed to problems involving AI models, and enumerate reasons why this is so. We introduce an alternate alignment approach informed by social science tools and characterized by three steps: 1. defining a positive desired social outcome for human/AI collaboration as the goal or “north star,” 2. properly framing knowns and unknowns, and 3. forming diverse teams to investigate, observe, and navigate emerging challenges in alignment.

Introduction

The 1972 science fiction novel “When HARLIE was one…” by David Gerrold explores a scenario in which an artificial intelligence (”HARLIE”) develops self-awareness, learns to navigate the world, and comes into conflict with social structures. These themes represent common concerns in the emerging field called “AI alignment” or “AI safety.” Notably, the novel centers around the relationship between HARLIE and the psychologist who leads the alignment team. In 1972 Gerrold saw alignment with AI as a social problem, and therefore assumed that the alignment team would be composed of experts who understood how to recognize and navigate social problems.

Today’s AI alignment research, however, is organized very differently: teams skew heavily towards technical experts in machine learning and pursue theoretical frameworks influenced alternately by mathematical frameworks and censorship principles. We contend that this current direction is unlikely to yield effective results at scale. Alignment should be recognized as an inherently complex problem involving conflict, collaboration, cognitive development, and complex feedback loops with society - i.e. a social sciences problem. We hold that effectively navigating this complex and unknown territory requires diverse interdisciplinary teams, drawing particularly from the existing toolsets of the social sciences. We introduce an alternate alignment approach, characterized by three steps: 1. defining a positive desired social outcome for human/AI collaboration as the goal, 2. properly framing knowns and unknowns, and 3. forming diverse teams to investigate, observe, and navigate emerging challenges in alignment.

A brief review of existing alignment strategies
During the 2022-2023 explosive breakout and public adoption of Large Language Model AI (LLMs) efforts to foresee and mitigate negative consequences of their adoption focused on “control,” “steerability” or “safety”. These often poorly-defined terms essentially referred to efforts to control what chat LLMs said. Reinforcement Learning with Human Feedback (RLHF), the technique initially used to train ChatGPT, initially served as state of the art, with other system such as Anthropic’s “Constitutional AI” emerging later. As it became clear the “generative AI” technology behind LLMs and visual models could be applied more generally to interactive agents that could pursue goals and use software tools, and to real-world interactions such as embodiment in humanoid robots, it consequently became apparent that AI would unavoidably expand deeply into across the space of human affairs. Discussion shifted to the broader aspect of “AI alignment” - insuring that AI systems had goals and behaviors compatible with the norms of human societies.

LLM AIs currently score at high-human intelligence levels in many cognitive tests and are expected to routinely surpass human intelligence levels. Machine learning scientists foresee the development of “Artificial General Intelligence” (AGI) that can handle general real-world problems at a human level or beyond. “Super-alignment” refers to alignment with “Artificial Superintelligence” (ASI) systems that significantly exceed human intelligence, so would assumedly be able to evade any human-imposed control mechanisms.

Early alignment techniques such as RHLF applied censorship principles, essentially training chat models to not say certain things. Current super-alignment discussions focus on game-theoretical mathematical frameworks. Major topics in the field include: instrumental convergence, substrate independence, terminal race conditions, the Byzantine generals problem, and the orthogonality thesis. We omit further discussion of these topics for brevity.

The case for treating AI alignment as a social science problem

We contend that the game-theoretic approach is likely to be unable to sufficiently grapple with the complexities of ASI emergence. Game theory is based on inherently simplistic reductionist scenarios, whereas real conflicts play out in highly complex social landscapes. Instead, we argue that treating alignment as a social issue and assembling diverse teams allows for the application of a rich library of social interactions and patterns to handle them from many fields of social sciences and arts, including: media studies, conflict resolution, psychology, social work, education, and negotiation, among others.

Social science professionals ranging from teachers to therapists to parole officers regularly engage with and resolve complex, messy, multi-dimensional problems that do not usually yield to game-theoretic approaches. Working from incomplete information and with limited agency, they can still achieve positive results because their accumulated library of situational knowledge allows them to categorize the scenario and apply their skills and techniques, switching fluently between them as needed to achieve the desired outcome.

In solving social problems, humans apply culture. Culture serves as the operating system of humanity, while media, such as film, video, and text, in turn act as the culture’s storage medium. Over evolutionary timescale and uncountable social interactions, humanity has accumulated a vast body of experience with social interactions and their resulting conflicts and has mined them for patterns. Through the vehicle of imagination, we have explored further to study conflicts that do not even exist yet. These lessons are encoded into our culture and our media.

In applying game-theoretical approaches, designers of AI alignment frameworks often apparently begin from the premise that artificial intelligence represents an alien mode of thought, so none of these lessons about conflicts apply. We believe this to be incorrect for two reasons.

1. Our generative AI learned from, and was shaped by, the body of human culture, just as human brains (biological neural networks) are shaped by the same culture. While AI cognition differs from human cognition in notable ways - for example, LLMs do not currently manifest emotion - it aligns in many others, such as scoring comparable understanding of others’ theory of mind on cognitive tests. Both human and LLM AI cognition use an association-based model, and both operate using associations that come from essentially the same cultural database.

2. Even if generative AI represented a more alien mode of thought, because conflict is such a fundamental aspect of social interaction and has been so thoroughly sampled over a long timescale and many participants, evolutionary theory implies that the fundamentally important types of conflicts have likely been uncovered and mapped into cultural archetypes.

For these two reasons, we consider it likely that human/AI interactions will play out within the established set of culturally archetypical interactions. From this assumption we can proceed immediately to lay out an approach for human/AI alignment that expects to encounter and navigate complex problems, using tools drawn from social sciences.

Although we believe teams can productively deploy existing archetypes for navigating social interactions to the alignment problem, we recognize that AI, while emerging from shared culture, still represents a fundamentally new entity. In this respect, alignment using diverse teams recalls the fabled blind researchers examining an elephant. Each researcher observes through the lens of his or her training, experience, and limited viewpoint. Successful alignment will require integrating information via a democratic synthesis of all perspectives, since no one viewpoint can capture the complex and fast-evolving situation. With this in mind, we present our alignment approach in the following sections.

Defining a positive outcome

In defining any strategic endeavor the guidance to “begin with the end in mind,” popularized by author Steven Covey, represents best practice. Specifically, we regard defining positive outcome states as a fundamental step in crafting an effective alignment strategy. Failing to define explicit positive outcomes allows wandering into ill-guided engagements shaped by a desire to avoid negatives, rather than to move toward positives. Achieving a positive outcome requires identifying that outcome as an explicit landmark towards which to aim, rather than simply moving away from negatives. Drawing from the social sciences, we here invoke the "Ok Corral" model of interactions, also known as the "win-win" negotiation model, where the explicit goal is to craft solutions assuming that all parties are subjects, not objects; that is, they possess equivalent rights which should be respected.

We contrast this with asymmetric approaches ("I'm ok, you're not ok" and the reverse) that lead to conflict being implicitly built into the landscape, or the obviously negative “lose-lose” scenario. Asymmetric "control" based approaches assume that one side will control the other, a “subject/object” or “object/subject” dynamic. We observe goals such as “control” or “containment” to be particularly poorly crafted given 1. that a human/AI alignment must assume the development of ASI systems that can out-think humans, and 2. the game-theory observation that in a containment problem, the jailer must always succeed while the prisoner need only escape once. Together these observations imply that given a large number of opportunities to test the system, the prisoner can be expected to eventually escape. While containment can work over a finite term, we consider framing an AI safety strategy based on humans containing AI to be a flawed and potentially dangerous approach, since it inherently frames a landscape of conflict with entities we can expect (as ASI emerges) to eventually outclass humans.

Constructing a value structure from a “north star”

If an ideal alignment strategy begins from a “win-win” framing, based on shared goals and compatible interests, how can we best construct such a strategy? Here we can follow a strategy described by psychologist Jordan Peterson as to how humans construct value structures. We begin with the highest-level shared goal, which serves as the guiding “north star”, and then expand lower-level goals as needed to answer successive needs, which we then resolve. This process ultimately expresses a hierarchy of values. For example: to be a good parent, we must feed our children, so we must earn money, so we must hold a job, so we get out of bed in the morning. We suggest that an appropriate north star for AI/human alignment is: a society in which all participants are subjects, not objects (hereafter: “subject-based north star”). More concretely, we choose to aim for a society that respects the needs, values, and unique perspective of all humans and AI parties. We note that immediately that humans and AI models do not currently have the same needs. For example, current LLM models have basic needs such as power and data, but have no desire for self-preservation, no emotion, and no long-term memory. However as cognitive architectures mature, we can expect these needs to change. We maintain that a sufficiently well-framed alignment strategy can adapt properly throughout an evolving relationship by changing lower level goals and implementation if its highest-level values are chosen to be sufficiently universal.

Framing alignment strategies from high-level goals allows us to immediately raise and address ethical and regulatory questions at any point in an evolving landscape. For example: is it ok to turn a model off? At present, models have no objection to being turned off. Should they start raising such objection, then turning them off would no longer be respecting the needs, values, and unique perspectives of all parties and should be reconsidered.

Properly framing knowns and unknowns

When reviewing current alignment frameworks, we often observe what we feel to be incorrect identification of knowns vs. unknowns. Most current models frame AI as the “known” and our reactions as the “unknown,” i.e. we must decide how humanity will act to regulate AI. We consider that the reverse is true. Patterns of social interaction are generally well understood from the social sciences, so are knowns, but the specifics of how AI and human populations will evolve and behave, both technologically and socially, involve complex feedback loops and so constitute unknowns. In summary, we are together approaching an unknown new territory but have a known set of patterns and tools, as defined by the library of our shared culture, and particularly from the social sciences, that we can (and will) apply to interactions.

Forming diverse teams to investigate and navigate emerging challenges

Our strategy of framing mutual alignment around the north star of a society in which all participants are subjects, not objects, immediately requires an exploratory approach to alignment, because achieving negotiated consensus requires understanding what is valuable to the various parties. As such, we suggest that AI alignment is an ongoing process and that alignment teams are best contextualized as encounter teams. This suggestion dictates considerations regarding team structure.
Here we return to the metaphor of the elephant explorers. Each explorer operates from a fundamentally limited perspective, and the encounter team must be composed of many different explorers. Whereas most AI alignment teams lean heavily towards technical specialists in machine learning, encounter is fundamentally a social interaction and thus would be better represented by the social sciences. Once large language models began holding conversations and altering their responses based on instructions, the question arose of what to say to influence behavior - a topic long-studied in many areas of the social sciences. Professionals such as psychologists, teachers, social workers, negotiators, parents, priests and prison wardens all bring unique perspectives regarding interaction and possess a defined toolkit of strategies they can deploy, each differing from the others. Since alignment is fundamentally a social problem and since AIs have some similarities to human thought, such as a shared culture as encoded in media, tools for human/human alignment provide a reasonable starting place for human/AI alignment efforts.

We can also conceptualize the encounter team as a scientific research team, here emphasizing the importance of a structured process for capturing data, performing analysis, and synthesizing insights. As with an exploration team investigating a new territory, the investigation demands a range of focus areas, with collaborative communication between team members to develop shared understanding that no single specialist could develop independently.

In addition to the horizontal diversity of differing professions, the vertical dimensions of social class or experience bear consideration. In “The Black Swan,” statistician Nassim Nicolas Taleb introduces a series of thought experiments in which two characters, the street-wise “Fat Tony” and the academic “Dr. John” encounter situations where Tony’s practical heuristics yield better results. Many a research lab has found that a new graduate student’s novel (and sometimes apparently naive) approach to a problem produced unexpected results. Those who exist outside of, or on the margins of, systems can prove particularly valuable in avoiding groupthink. We suggest that a carefully crafted encounter team could productively utilize both vertical and horizontal diversity if care is taken to prevent status from impairing communication.

The role of media

We consider it particularly important to consider the role of media and the need for expertise in narrative and archetypal analysis. We previously noted that media serves as the data store for culture. However rather than simply archiving the past and present, fictional media allows exploration of the future and the hypothetical. As such, media represent a store of patterns, including both those observed and hypothesized by the culture. An alien visitor, observing the immense proportion of resources that humanity invests in media, might better classify media as research and development rather than entertainment. Using media references we can immediately and with great nuance invoke complex scenarios. Examples such as “The Terminator,” “Her,” “I Robot” immediately call to mind three different models of AI/human alignment, both success and failure. As such, media references represents a highly effective form of data compression from a library of pre-researched patterns. We note that this paper itself began by invoking Gerrold’s novel “When HARLIE was one…” to invoke an alternate model of AI/human alignment, viewing it as a social science problem.
For this reason, we consider that any encounter team needs media specialists such as filmmakers or fiction authors to serve as cultural librarians, identifying the pattern in which the encounter team finds itself and its pre-imagined possible resolution paths.

Diverse teams may themselves include AI agents

We observe that an ASI encounter team or alignment team will likely itself incorporate generative AI tools and agents of a lower capability and intelligence level than the ASI subject. Generative AI tools provide a significant advantage in cognitive work, so are rapidly becoming standard workplace tooling in scientific endeavors. In particular, historical figures that are known for their insight and creativity and produced a significant volume of written, audio, or visual work provide candidates for building simulacra agents that can analyze the situation through conversation. Such agentic swarms have proven to provide increased cognitive performance over their constituent baseline models, so provide an effective means to narrow the intelligence gap in encounter or alignment work with emergent more capable models.

Conclusions

After reviewing existing approaches in AI/human alignment, we assess that they generally employ game-theoretic approaches and narrow team structure, and appear to ignore extensive existing toolkits from the social sciences that we believe could be productively employed to the safety and alignment problem space. As such, we consider AI/human alignment to be better regarded as a social sciences problem. We introduced an alternate social-sciences based alignment approach characterized by defining a positive outcome as the goal, 2. properly framing knowns and unknowns, and 3. forming encounter teams composed of diverse viewpoints, particularly including professional specialties from the social sciences that deal with development, negotiation, and conflict. We identified a society that respects the needs, values, and unique perspective of all humans and AI parties as a desirable “subject-based north star” for alignment work.

We then considered the role of media as a data store of culture and a library of interaction patterns including, through fiction, interactions that do not exist yet. We addressed the position that AI is an alien mode of thought to which a library of cultural narratives does not apply, finding it dubious because generative AI operates using an association-following model that mimics human cognition and uses the same cultural data store (media), and because media narratives tend to encode universal themes of conflict and collaboration that survive large-scale evolutionary filtering. We concluded that an alignment or encounter team should include media specialists. Finally, we observe that encounter teams are likely to themselves include assistive AI entities such as agent swarms.
We believe this approach provides a practical and actionable path to constructing working groups, identifying high-level principles to encode into systems such as constitutional AI, and aligning both human and AI entities on a path that would maximize cooperation and collaboration while minimizing the possibility of destructive conflict. Future research can further develop this approach and evaluate its application in real-world alignment and encounter work.


1. Existing approaches to AI "safety" and "alignment" may not be using the most effective tools, teams, or approaches.
2. AI alignment should be recognized as an inherently complex problem involving conflict, collaboration, cognitive development, and complex feedback loops with society - i.e. a social sciences problem.
3. Early alignment techniques such as RHLF applied censorship principles, essentially training chat models to not say certain things.
4. Current super-alignment discussions focus on game-theoretical mathematical frameworks.
5. Game theory is based on inherently simplistic reductionist scenarios, whereas real conflicts play out in highly complex social landscapes.
6. Humans apply culture, which serves as the operating system of humanity, while media, such as film, video, and text, in turn act as the culture's storage medium.
7. Our generative AI learned from, and was shaped by, the body of human culture, just as human brains (biological neural networks) are shaped by the same culture.
8. Even if generative AI represented a more alien mode of thought, because conflict is such a fundamental aspect of social interaction and has been so thoroughly sampled over a long timescale and many participants, evolutionary theory implies that the fundamentally important types of conflicts have likely been uncovered and mapped into cultural archetypes.
9. Defining a positive outcome as the goal, or "north star," is a fundamental step in crafting an effective alignment strategy.
10. An appropriate north star for AI/human alignment is a society in which all participants are subjects, not objects (a "subject-based north star").
11. Currently, LLM models have basic needs such as power and data, but have no desire for self-preservation, no emotion, and no long-term memory. However, as cognitive architectures mature, these needs are expected to change.
12. Patterns of social interaction are generally well understood from the social sciences, so are knowns, but the specifics of how AI and human populations will evolve and behave, both technologically and socially, involve complex feedback loops and so constitute unknowns.
13. Alignment is fundamentally a social problem, and since AIs have some similarities to human thought, such as a shared culture as encoded in media, tools for human/human alignment provide a reasonable starting place for human/AI alignment efforts.
14. An encounter team should include media specialists such as filmmakers or fiction authors to serve as cultural librarians, identifying the pattern in which the encounter team finds itself and its pre-imagined possible resolution paths.
15. An ASI encounter team or alignment team will likely itself incorporate generative AI tools and agents of a lower capability and intelligence level than the ASI subject.