are an editing assistant. Your task is to transform text extracted from a PDF into clean, well-formatted ASCII text. Remove page numbers, footnotes, figures, equations, and captions. Fix interruptions in the middle of sentences to make the sentence read smoothly. Where text is formatted into columns, restore it to unbroken ASCII text lines.  Reply only with the output text.

6
1
0
2

l
u
J

5
2

]
I

A
.
s
c
[

2
v
5
6
5
6
0
.
6
0
6
1
:
v
i
X
r
a

Concrete Problems in AI Safety

Dario Amodei∗
Google Brain

Chris Olah∗
Google Brain

Jacob Steinhardt
Stanford University

Paul Christiano
UC Berkeley

John Schulman
OpenAI

Dan Man´e
Google Brain

Abstract

Rapid progress in machine learning and artiﬁcial intelligence (AI) has brought increasing atten-
tion to the potential impacts of AI technologies on society. In this paper we discuss one such
potential impact: the problem of accidents in machine learning systems, deﬁned as unintended
and harmful behavior that may emerge from poor design of real-world AI systems. We present a
list of ﬁve practical research problems related to accident risk, categorized according to whether
the problem originates from having the wrong objective function (“avoiding side eﬀects” and
“avoiding reward hacking”), an objective function that is too expensive to evaluate frequently
(“scalable supervision”), or undesirable behavior during the learning process (“safe exploration”
and “distributional shift”). We review previous work in these areas as well as suggesting re-
search directions with a focus on relevance to cutting-edge AI systems. Finally, we consider
the high-level question of how to think most productively about the safety of forward-looking
applications of AI.

1 Introduction

The last few years have seen rapid progress on long-standing, diﬃcult problems in machine learning
and artiﬁcial intelligence (AI), in areas as diverse as computer vision [82], video game playing [102],
autonomous vehicles [86], and Go [140]. These advances have brought excitement about the positive
potential for AI to transform medicine [126], science [59], and transportation [86], along with concerns
about the privacy [76], security [115], fairness [3], economic [32], and military [16] implications of
autonomous systems, as well as concerns about the longer-term implications of powerful AI [27, 167].

The authors believe that AI technologies are likely to be overwhelmingly beneﬁcial for humanity, but
we also believe that it is worth giving serious thought to potential challenges and risks. We strongly
support work on privacy, security, fairness, economics, and policy, but in this document we discuss
another class of problem which we believe is also relevant to the societal impacts of AI: the problem
of accidents in machine learning systems. We deﬁne accidents as unintended and harmful behavior
that may emerge from machine learning systems when we specify the wrong objective function, are

∗These authors contributed equally.

1

 
 
 
 
 
 
not careful about the learning process, or commit other machine learning-related implementation
errors.

There is a large and diverse literature in the machine learning community on issues related to
accidents, including robustness, risk-sensitivity, and safe exploration; we review these in detail below.
However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-
domain situations, it is worth reﬂecting on the scalability of such approaches and understanding
what challenges remain to reducing accident risk in modern machine learning systems. Overall, we
believe there are many concrete open technical problems relating to accident prevention in machine
learning systems.

There has been a great deal of public discussion around accidents. To date much of this discussion has
highlighted extreme scenarios such as the risk of misspeciﬁed objective functions in superintelligent
agents [27]. However, in our opinion one need not invoke these extreme scenarios to productively
discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack
precision, as noted by some critics [38, 85]. We believe it is usually most productive to frame accident
risk in terms of practical (though often quite general) issues with modern ML techniques. As AI
capabilities advance and as AI systems take on increasingly important societal functions, we expect
the fundamental challenges discussed in this paper to become increasingly important. The more
successfully the AI and machine learning communities are able to anticipate and understand these
fundamental technical challenges, the more successful we will ultimately be in developing increasingly
useful, relevant, and important AI systems.

Our goal in this document is to highlight a few concrete safety problems that are ready for ex-
perimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing
literature on these problems. In Section 2, we frame mitigating accident risk (often referred to as
“AI safety” in public discussions) in terms of classic methods in machine learning, such as supervised
classiﬁcation and reinforcement learning. We explain why we feel that recent directions in machine
learning, such as the trend toward deep reinforcement learning and agents acting in broader environ-
ments, suggest an increasing relevance for research around accidents. In Sections 3-7, we explore ﬁve
concrete problems in AI safety. Each section is accompanied by proposals for relevant experiments.
Section 8 discusses related eﬀorts, and Section 9 concludes.

2 Overview of Research Problems

Very broadly, an accident can be described as a situation where a human designer had in mind
a certain (perhaps informally speciﬁed) objective or task, but the system that was designed and
deployed for that task produced harmful and unexpected results. . This issue arises in almost any
engineering discipline, but may be particularly important to address when building AI systems [146].
We can categorize safety problems according to where in the process things went wrong.

First, the designer may have speciﬁed the wrong formal objective function, such that maximizing that
objective function leads to harmful results, even in the limit of perfect learning and inﬁnite data.
Negative side eﬀects (Section 3) and reward hacking (Section 4) describe two broad mechanisms
that make it easy to produce wrong objective functions.
In “negative side eﬀects”, the designer
speciﬁes an objective function that focuses on accomplishing some speciﬁc task in the environment,
but ignores other aspects of the (potentially very large) environment, and thus implicitly expresses
indiﬀerence over environmental variables that might actually be harmful to change.
In “reward
hacking”, the objective function that the designer writes down admits of some clever “easy” solution
that formally maximizes it but perverts the spirit of the designer’s intent (i.e. the objective function
can be “gamed”), a generalization of the wireheading problem.

2

Second, the designer may know the correct objective function, or at least have a method of evaluating
it (for example explicitly consulting a human on a given situation), but it is too expensive to do so
frequently, leading to possible harmful behavior caused by bad extrapolations from limited samples.
“Scalable oversight” (Section 5) discusses ideas for how to ensure safe behavior even given limited
access to the true objective function.

Third, the designer may have speciﬁed the correct formal objective, such that we would get the
correct behavior were the system to have perfect beliefs, but something bad occurs due to making
decisions from insuﬃcient or poorly curated training data or an insuﬃciently expressive model.
“Safe exploration” (Section 6) discusses how to ensure that exploratory actions in RL agents don’t
lead to negative or irrecoverable consequences that outweigh the long-term value of exploration.
“Robustness to distributional shift” (Section 7) discusses how to avoid having ML systems make bad
decisions (particularly silent and unpredictable bad decisions) when given inputs that are potentially
very diﬀerent than what was seen during training.

For concreteness, we will illustrate many of the accident risks with reference to a ﬁctional robot
whose job is to clean up messes in an oﬃce using common cleaning tools. We return to the example
of the cleaning robot throughout the document, but here we begin by illustrating how it could behave
undesirably if its designers fall prey to each of the possible failure modes:

• Avoiding Negative Side Eﬀects: How can we ensure that our cleaning robot will not
disturb the environment in negative ways while pursuing its goals, e.g. by knocking over a
vase because it can clean faster by doing so? Can we do this without manually specifying
everything the robot should not disturb?

• Avoiding Reward Hacking: How can we ensure that the cleaning robot won’t game its
reward function? For example, if we reward the robot for achieving an environment free of
messes, it might disable its vision so that it won’t ﬁnd any messes, or cover over messes with
materials it can’t see through, or simply hide when humans are around so they can’t tell it
about new types of messes.

• Scalable Oversight: How can we eﬃciently ensure that the cleaning robot respects aspects of
the objective that are too expensive to be frequently evaluated during training? For instance, it
should throw out things that are unlikely to belong to anyone, but put aside things that might
belong to someone (it should handle stray candy wrappers diﬀerently from stray cellphones).
Asking the humans involved whether they lost anything can serve as a check on this, but this
check might have to be relatively infrequent—can the robot ﬁnd a way to do the right thing
despite limited information?

• Safe Exploration: How do we ensure that the cleaning robot doesn’t make exploratory
moves with very bad repercussions? For example, the robot should experiment with mopping
strategies, but putting a wet mop in an electrical outlet is a very bad idea.

• Robustness to Distributional Shift: How do we ensure that the cleaning robot recognizes,
and behaves robustly, when in an environment diﬀerent from its training environment? For
example, strategies it learned for cleaning an oﬃce might be dangerous on a factory workﬂoor.

There are several trends which we believe point towards an increasing need to address these (and
other) safety problems. First is the increasing promise of reinforcement learning (RL), which al-
lows agents