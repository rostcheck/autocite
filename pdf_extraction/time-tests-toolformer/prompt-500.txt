are an editing assistant. Your task is to transform text extracted from a PDF into clean, well-formatted ASCII text. Remove page numbers, footnotes, figures, equations, and captions. Fix interruptions in the middle of sentences to make the sentence read smoothly. Where text is formatted into columns, restore it to unbroken ASCII text lines.  Reply only with the output text.

Toolformer: Language Models Can Teach Themselves to Use Tools

Timo Schick

Jane Dwivedi-Yu Roberto Dessì† Roberta Raileanu

Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom

Meta AI Research †Universitat Pompeu Fabra

3
2
0
2

b
e
F
9

]
L
C
.
s
c
[

1
v
1
6
7
4
0
.
2
0
3
2
:
v
i
X
r
a

Abstract

Language models (LMs) exhibit remarkable
abilities to solve new tasks from just a few
examples or textual instructions, especially at
scale. They also, paradoxically, struggle with
basic functionality, such as arithmetic or fac-
tual lookup, where much simpler and smaller
In this paper, we show that
models excel.
LMs can teach themselves to use external tools
via simple APIs and achieve the best of both
worlds. We introduce Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriﬁc-
ing its core language modeling abilities.

1

Introduction

Large language models achieve impressive zero-
and few-shot results on a variety of natural lan-
guage processing tasks (Brown et al., 2020; Chowd-
hery et al., 2022, i.a.) and show several emergent
capabilities (Wei et al., 2022). However, all of
these models have several inherent limitations that
can at best be partially addressed by further scal-
ing. These limitations include an inability to access
up-to-date information on recent events (Komeili
et al., 2022) and the related tendency to hallucinate
facts (Maynez et al., 2020; Ji et al., 2022), difﬁcul-
ties in understanding low-resource languages (Lin
et al., 2021), a lack of mathematical skills to per-
form precise calculations (Patel et al., 2021) and an
unawareness of the progression of time (Dhingra
et al., 2022).

Figure 1: Exemplary predictions of Toolformer. The
model autonomously decides to call different APIs
(from top to bottom: a question answering system,
a calculator, a machine translation system, and a
Wikipedia search engine) to obtain information that is
useful for completing a piece of text.

A simple