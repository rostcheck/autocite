are an editing assistant. Your task is to transform text extracted from a PDF into clean, well-formatted ASCII text. Remove page numbers, footnotes, figures, equations, and captions. Fix interruptions in the middle of sentences to make the sentence read smoothly. Where text is formatted into columns, restore it to unbroken ASCII text lines.  Reply only with the output text.

Toolformer: Language Models Can Teach Themselves to Use Tools

Timo Schick

Jane Dwivedi-Yu Roberto Dessì† Roberta Raileanu

Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom

Meta AI Research †Universitat Pompeu Fabra

3
2
0
2

b
e
F
9

]
L
C
.
s
c
[

1
v
1
6
7
4
0
.
2
0
3
2
:
v
i
X
r
a

Abstract

Language models (LMs) exhibit remarkable
abilities to solve new tasks from just a few
examples or textual instructions, especially at
scale. They also, paradoxically, struggle with
basic functionality, such as arithmetic or fac-
tual lookup, where much simpler and smaller
In this paper, we show that
models excel.
LMs can teach themselves to use external tools
via simple APIs and achieve the best of both
worlds. We introduce Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substan-
tially improved zero-shot performance across
a variety of downstream tasks, often competi-
tive with much larger models, without sacriﬁc-
ing its core language modeling abilities.

1

Introduction

Large language models achieve impressive zero-
and few-shot results on a variety of natural lan-
guage processing tasks (Brown et al., 2020; Chowd-
hery et al., 2022, i.a.) and show several emergent
capabilities (Wei et al., 2022). However, all of
these models have several inherent limitations that
can at best be partially addressed by further scal-
ing. These limitations include an inability to access
up-to-date information on recent events (Komeili
et al., 2022) and the related tendency to hallucinate
facts (Maynez et al., 2020; Ji et al., 2022), difﬁcul-
ties in understanding low-resource languages (Lin
et al., 2021), a lack of mathematical skills to per-
form precise calculations (Patel et al., 2021) and an
unawareness of the progression of time (Dhingra
et al., 2022).

Figure 1: Exemplary predictions of Toolformer. The
model autonomously decides to call different APIs
(from top to bottom: a question answering system,
a calculator, a machine translation system, and a
Wikipedia search engine) to obtain information that is
useful for completing a piece of text.

A simple way to overcome these limitations of
today’s language models is to give them the abil-
ity to use external tools such as search engines,
calculators, or calendars. However, existing ap-
proaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-speciﬁc settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), hinder-
ing a more widespread adoption of tool use in LMs.
Therefore, we propose Toolformer, a model that
learns to use tools in a novel way, which fulﬁlls the
following desiderata:

• The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. This is impor-

The New England Journal of Medicine is a registered trademark of [QA(“Who is the publisher of The New England Journal of Medicine?”) → Massachusetts Medical Society] the MMS.Out of 1400 participants, 400 (or [Calculator(400 / 1400) → 0.29] 29%) passed the test. The name derives from “la tortuga”, the Spanish word for [MT(“tortuga”) → turtle] turtle.The Brown Act is California’s law [WikiSearch(“Brown Act”) → The Ralph M. Brown Act is an act of the California State Legislature that guarantees the public's right to attend and participate in meetings of local legislative bodies.] that requires legislative bodies, like city councils, to hold their meetings open to the public. 
 
 
 
 
 
Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we ﬁrst
sample a position i and corresponding API call candidates c1
i . We then execute these API calls and
ﬁlter out all calls which do not reduce the loss Li over the next tokens. All remaining API calls are interleaved
with the original text, resulting in a new text x∗.

i , . . . , ck

i , c2

tant not only because of the costs associated
with such annotations, but also because what
humans ﬁnd useful may be different from
what a model ﬁnds useful.

• The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool.
In contrast to
existing approaches, this enables a much more
comprehensive use of tools that is not tied to
speciﬁc tasks.

Our approach for achieving these goals is based
on the recent idea of using large LMs with in-
context learning (Brown et al., 2020) to generate
entire datasets from scratch (Schick and Schütze,
2021b; Honovich et al., 2022; Wang et al., 2022):
Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
ﬁnetune the LM itself on the API calls that it con-
siders useful. As illustrated in Figure 1, through
this simple approach, LMs can learn to control a va-
riety of tools, and to choose for themselves which
tool to use when and how.

As our approach is agnostic of the dataset be-
ing used, we can apply it to the exact same dataset
that was used to pretrain a model in the ﬁrst place.
This ensures that the model does not lose any
of its generality and language modeling abilities.
We conduct experiments on a variety of differ-
ent downstream tasks, demonstrating that after
learning to use tools, Toolformer, which is based
on a pretrained GPT-J model (Wang and Komat-
suzaki, 2021) with 6.7B parameters, achieves much
stronger zero-shot results, clearly outperforming a
much larger GPT-3 model (Brown et al., 2020) and

several other baselines on various tasks.

2 Approach

Our aim is to equip a language model M with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call.

We represent each API call as a tuple c = (ac, ic)
where ac is the name of the API and ic is the cor-
responding input. Given an API call c with a cor-
responding result r, we denote the linearized se-
quences of the API call not including and including
its result, respectively, as:

e(c) = <API> ac(ic) </API>
e(c, r) = <API> ac(ic) → r </API>

where “<API>”, “</API>” and “→” are special
tokens.1 Some examples of linearized API calls
inserted into text sequences are shown in Figure 1.
Given a dataset C = {x1, . . . , x|C|} of plain
texts, we ﬁrst convert this dataset into a dataset
C∗ augmented with API calls. This is done in three
steps, illustrated in Figure 2: First, we exploit the
in-context learning ability of M to sample a large
number of potential API calls. We then execute
these API calls and ﬁnally check whether the ob-
tained responses are helpful for predicting future
tokens; this is used as a ﬁltering criterion. After
ﬁltering, we merge API calls for different tools,
resulting in the augmented dataset C∗, and ﬁnetune

1In practice, we use the token sequences “ [”, “]” and
“->” to represent “<API>”, “</API>” and “→”, respec-
tively. This enables our approach to work without modifying
the existing LM’s vocabulary. For reasons of readability, we
still refer to them as “<API>”, “</API>” and “→” through-
out this section.

x1:i-1  = Pittsburgh is              also known as   xi:n = the Steel Cityx* = Pittsburgh is         also known as        [QA(What …?         → Steel City)]         the Steel City.ci1 = What other name is          Pittsburgh known by?ci2 = Which country is         Pittsburgh in?ri1 = Steel City ri2 = United StatesLi(ci1 → Steel City) < min(Li(ci1 → ε), Li(ε))Li(ci2 → United States) > min(Li(ci2 → ε), Li(ε))1 Sample API Calls2 Execute API Calls3 Filter API CallsLM DatasetLM Dataset with API CallsExecuting API Calls As a next step, we execute
all API calls generated by M to obtain the corre-
sponding results. How this is done depends entirely
on the API itself – for example, it can involve call-
ing another neural network, executing a Python
script or using a retrieval system to perform search
over a large corpus. The response for each API call
ci needs to be a single text sequence ri.

Filtering API Calls Let i be the position of the
API call ci in the sequence x = x1, . . . , xn, and let
ri be the response from the API. Further, given a
sequence (wi | i ∈ N) of weights, let

Li(z) = −

n
(cid:88)

j=i

wj−i · log pM (xj | z, x1:j−1)

be the weighted cross entropy loss for M over the
tokens xi, . . . , xn if the model is preﬁxed with z.
We compare two different instantiations of this loss:

L+
L−

i = Li(e(ci, ri))
i = min (Li(ε), Li(e(ci, ε)))

where ε denotes an empty sequence. The former is
the weighted loss over all tokens xi, . . . , xn if the
API call and its result are given to M as a preﬁx;3
the latter is the minimum of the losses obtained
from (i) doing no API call at all and (ii) doing an
API call, but not providing the response. Intuitively,
an API call is helpful to M if providing it with both
the input and the output of this call makes it easier
for the model to predict future tokens, compared to
not receiving the API call at all, or receiving only
its input. Given a ﬁltering threshold τf , we thus
only keep API calls for which

i − L+
L−

i ≥ τf

holds, i.e., adding the API call and its result reduces
the loss by at least τf , compared to not doing any
API call or obtaining no result from it.

Model Finetuning After sampling and ﬁltering
calls for all APIs, we ﬁnally merge the remaining
API calls and interleave them with the original
inputs. That is, for an input text x = x1, . . . , xn
with a corresponding API call and result (ci, ri) at
position i, we construct the new sequence x∗ =

3We provide e(ci, ri) as a preﬁx instead of inserting it at
position i because M is not yet ﬁnetuned on any examples
containing API calls, so inserting it in the middle of x would
interrupt the ﬂow and not align with patterns in the pretraining
corpus, thus hurting perplexity.

Figure 3: An exemplary prompt P (x) used to generate
API calls for the question answering tool.

M itself on this dataset. Each of these steps is
described in more detail below.

Sampling API Calls For each API, we write a
prompt P (x) that encourages the LM to anno-
tate an example x = x1, . . . , xn with API calls.
An example of such a prompt for a question an-
swering tool is shown in Figure 3; all prompts
used are shown in Appendix A.2. Let pM (zn+1 |
z1, . . . , zn) be the probability that M assigns to
token zn+1 as a continuation for the sequence
z1, . . . , zn. We ﬁrst sample up to k candidate posi-
tions for doing API calls by computing, for each
i ∈ {1, . . . , n}, the probability

pi = pM (<API> | P (x), x1:i−1)

that M assigns to starting an API call at position
i. Given a sampling threshold τs, we keep all po-
sitions I = {i | pi > τs}; if there are more than k
such positions, we only keep the top k.

For each position i ∈ I, we then obtain up to m
API calls c1
i by sampling from M given the
sequence [P (x), x1, . . . , xi−1, <API>] as a preﬁx
and </API> as an end-of-sequence token.2

i , . . . , cm

2We discard all examples where M does not generate the

</API> token.

Your task is to add calls to a Question Answering API to a piece of text. The questions should help you get information required to complete the text. You can call the API by writing "[QA(question)]" where "question" is the question you want to ask. Here are some examples of API calls:Input: Joe Biden was born in Scranton, Pennsylvania.Output: Joe Biden was born in [QA("Where was Joe Biden born?")] Scranton, [QA("In which state is Scranton?")] Pennsylvania.Input: Coca-Cola, or Coke, is a carbonated soft drink manufactured by the Coca-Cola Company.Output: Coca-Cola, or [QA("What other name is Coca-Cola known by?")] Coke, is a carbonated soft drink manufactured by [QA("Who manufactures Coca-Cola?")] the Coca-Cola Company.Input: xOutput:x1:i−1, e(ci, ri), xi:n; we proceed analogously for
texts with multiple API calls. Doing this for all x ∈
C results in the new dataset C∗ augmented with API
calls. We use this new dataset to ﬁnetune M , using
a standard language modeling objective. Crucially,
apart from inserted API calls the augmented dataset
C∗ contains the exact same texts as C, the original
dataset. As a consequence, ﬁnetuning M on C∗
exposes it to the same content as ﬁnetuning on C.
Moreover, as API calls are inserted in exactly those
positions and with exactly those inputs that help
M predict future tokens, ﬁnetuning on C∗ enables
the language model to decide when and how to use
which tool, based purely on its own feedback.

Inference When generating text with M after
ﬁnetuning with our approach, we perform regular
decoding until M produces the “→” token, indicat-
ing that it next expects the response for an API call.
At this point, we interrupt the decoding process,
call the appropriate API to get a response, and con-
tinue the decoding process after inserting both the
response and the </API> token.

3 Tools

We explore a variety of tools to address different
shortcomings of regular LMs. The only constraints
we impose on these tools is that (i) both their inputs
and outputs can be represented as text sequences,
and (ii) we can obtain a few demonstrations of
their intended use. Concretely, we explore the fol-
lowing ﬁve tools: a question answering system, a
Wikipedia search engine, a calculator, a calendar,
and a machine translation system. Some examples
of potential calls and return strings for the APIs
associated with each of these tools are shown in
Table 1. We brieﬂy discuss all tools below; further
details can be found in Appendix A.

Question Answering Our ﬁrst tool is a question
answering system based on another LM that can an-
swer simple factoid questions. Speciﬁcally, we use
Atlas (Izacard et al., 2022), a retrieval-augmented
LM ﬁnetuned on Natural Questions (Kwiatkowski
et al., 2019).

Calculator As a second tool, we use a calculator
that can perform simple numeric calculations; we
only support the four basic arithmetic operations.
Results are always rounded to two decimal places.

Wikipedia Search Our third tool is a search en-
gine that, given a search term, returns short text

snippets from Wikipedia. Compared to our ques-
tion answering tool, this search enables a model
to get more comprehensive information on a sub-
ject, but requires it to extract the relevant parts by
itself. As our search engine, we use a BM25 re-
triever (Robertson et al., 1995; Baeza-Yates et al.,
1999) that indexes the Wikipedia dump from KILT
(Petroni et al., 2021).

Machine Translation System Our fourth tool is
a machine translation system based on a LM that
can translate a phrase from any language into En-
glish. More concretely, we use the 600M parameter
NLLB (Costa-jussà et al., 2022) as our multilingual
machine translation model that works for 200 lan-
guages (including low-resource ones). The source
language is automatically detected using the fast-
Text classiﬁer (Joulin et al., 2016), while the target
language is always set to English.

Calendar Our ﬁnal tool is a calendar API that,
when queried, returns the current date without tak-
ing any input. This provides temporal context for
predictions that require some awareness of time.

4 Experiments

We investigate whether our approach enables a
model to use tools without any further supervision
and to decide for itself when and how to call which
of the available tools. To test this, we select a vari-
ety of downstream tasks where we assume at least
one of the considered tools to be useful, and evalu-
ate performance in zero-shot settings (Section 4.2).
Beyond that, we also ensure that our approach does
not hurt the model’s core language modeling abili-
ties; we verify this by looking at perplexity on two
language modeling datasets (Section 4.3). Finally,
we investigate how the ability to learn using tools
is affected by model size (Section 4.4).

4.1 Experimental Setup

Dataset Generation Throughout all of our ex-
periments, we use a subset of CCNet (Wenzek et al.,
2020) as our language modeling dataset C and GPT-
J (Wang and Komatsuzaki, 2021) as our language
model M . To reduce the computational cost of
annotating C with API calls, we deﬁne heuristics
for some APIs to get a subset of C for which API
calls are more likely to be helpful than for an av-
erage text. For example, we only consider texts
for the calculator tool if they contain at least three
numbers. Details of the heuristics used are given in

API Name

Example Input

Example Output

Question Answering Where was the Knights
of Columbus founded?

New Haven, Connecticut

Wikipedia Search

Fishing Reel Types

Spin ﬁshing > Spin ﬁshing is distinguished between ﬂy ﬁshing and bait
cast ﬁshing by the type of rod and reel used. There are two types of reels
used when spin ﬁshing, the open faced reel and the closed faced reel.

Calculator

Calendar

27 + 4 * 2

ε

35

Today is Monday, January 30, 2023.

Machine Translation

sûreté nucléaire

nuclear safety

Table 1: Examples of inputs and outputs for all APIs used.

API

Number of Examples
τf = 1.0

τf = 2.0

τf = 0.5

Question Answering
Wikipedia Search
Calculator
Calendar
Machine Translation

51,987
207,241
3,680
61,811
3,156

18,526
60,974
994
20,587
1,034

5,135
13,944
138
3,007
229

Table 2: Number of examples with API calls in C∗ for
different values of our ﬁltering threshold τf .

Appendix A. For obtaining C∗ from C, we perform
all steps described in Section 2 and additionally
ﬁlter out all examples for which all API calls were
eliminated in the ﬁltering step.4 For the weighting
function, we use

wt =

˜wt
s∈N ˜ws

(cid:80)

with ˜wt = max(0, 1 − 0.2 · t)

to make sure that API calls happen close to where
the information provided by the API is actually
helpful for the model. The thresholds τs and τf are
chosen individually for each tool to ensure a sufﬁ-
ciently larger number of examples; see Appendix A
for details. Table 2 shows relevant statistics of our
ﬁnal dataset augmented with API calls.

Model Finetuning We ﬁnetune M on C∗ using
a batch size of 128 and a learning rate of 1 · 10−5
with linear warmup for the ﬁrst 10% of training.
Details of our ﬁnetuning procedure are given in
Appendix B.

Baseline Models Throughout the remainder of
this section, we mainly compare